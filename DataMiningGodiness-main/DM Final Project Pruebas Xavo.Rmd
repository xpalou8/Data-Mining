---
title: "Main"
output:
  pdf_document: default
  html_document: default
date: '2022-12-11'
---

```{r setup, include=FALSE, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
knitr::opts_chunk$set(echo = TRUE)
library('corrplot')
library('ggplot2')
library('Hmisc')
library('moments')
library('dendextend')
library('countrycode')
library('ggthemes') # Load
library('dplyr')
library('tidyverse')
library('cluster')
library('factoextra')
library('e1071')
library('caret')
library('nnet')
library('randomForest')
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(pROC)
library(tree)
```

```{r}
salaries= read.csv("AI_MLsalaries.csv")
my_data <- salaries
```

```{r}
#summarizing the data
str(my_data)
summary(my_data)
```

```{r}
auxiliar <- my_data
my_data[sapply(my_data, is.character)] <- data.matrix(my_data[sapply(my_data, is.character)])
summary(my_data)
plot(my_data$salary_in_usd)
hist(my_data$salary_in_usd)
```


```{r}
#removing outliers
boxplot(my_data$salary_in_usd)
quartiles1 <- quantile(my_data$salary_in_usd, probs=c(.01, .90), na.rm = FALSE)
IQR <- IQR(my_data$salary_in_usd)
 
Lower1 <- quartiles1[1] - 1.5*IQR
Upper1 <- quartiles1[2] + 1.5*IQR
 
data_no_outlier <- subset(my_data,my_data$salary_in_usd > Lower1 & my_data$salary_in_usd < Upper1)
boxplot(data_no_outlier$salary_in_usd)
hist(data_no_outlier$salary_in_usd)
```


```{r}
plot(data_no_outlier$salary_in_usd)
```

```{r}
salaTitle <-  data_no_outlier[, c(4, 5)]
summary(salaTitle)
plot(salaTitle)
```



```{r}
summary(data_no_outlier$experience_level)
summary(data_no_outlier$employee_residence)
summary(data_no_outlier$employment_type)
summary(data_no_outlier$company_size)
summary(data_no_outlier$remote_ratio)
summary(data_no_outlier$job_title)
```

plot de los salarios por experiencia diferenciado en años
```{r}
plot(data_no_outlier$company_size, data_no_outlier$salary_in_usd , col=data_no_outlier$company_size, xlab="company_size", ylab="salary" )
```

```{r}
corrplot(cor(data_no_outlier), method = "color", type = "upper")
```

```{r}
#reiniciar los datos
data_no_outlier <- read.csv("AI_MLsalaries.csv",stringsAsFactors = TRUE)

#removing outliers
quartiles1 <- quantile(data_no_outlier$salary_in_usd, probs=c(.01, .90), na.rm = FALSE)
IQR <- IQR(data_no_outlier$salary_in_usd)
 
Lower1 <- quartiles1[1] - 1.5*IQR
Upper1 <- quartiles1[2] + 1.5*IQR
 
data_no_outlier <- subset(data_no_outlier,data_no_outlier$salary_in_usd > Lower1 & data_no_outlier$salary_in_usd < Upper1)

#eliminating columns
data_no_outlier["salary"] <- NULL
data_no_outlier["employee_residence"] <- NULL
data_no_outlier["salary_currency"] <- NULL

#eliminating not FT
head(data_no_outlier)
data_no_outlier <- data_no_outlier[data_no_outlier$employment_type == "FT",]

#remote_ratio as factor
data_no_outlier["remote_ratio"] <- as.factor(data_no_outlier$remote_ratio)
data_no_outlier["work_year"] <- as.factor(data_no_outlier$work_year)
#NR -> no remote work, PR -> partially remote, FR -> fully remtote
levels(data_no_outlier$remote_ratio) <- list(NR  = "0", PR = "50", FR = "100")

#grouping by continents
data_no_outlier$continent <- countrycode(sourcevar = data_no_outlier[, "company_location"],
                            origin = "iso2c",
                            destination = "continent")
data_no_outlier$continent[data_no_outlier$company_location == "CA" | data_no_outlier$company_location == "US"] <- "North America"
data_no_outlier$continent = as.factor(data_no_outlier$continent)

#gruouping jobs (for clustering approach)
data_no_outlier$job_title_grouped <- data_no_outlier$job_title
data_no_outlier[grepl("BI", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "BI Analyst"
data_no_outlier[grepl("Data Analy", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Analyst"
data_no_outlier[grepl("Sci", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Scientist"
data_no_outlier[grepl("Machine Learning", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "ML Engineer"
data_no_outlier[grepl("Data Engi", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("NLP", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Analytics", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Research", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("ETL", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Data Operations", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Computer Vision", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "3D Computer Vision Researcher"
data_no_outlier[grepl("Data Architect", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Architect"
data_no_outlier[grepl("Head of", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Manager"
data_no_outlier$job_title_grouped <- factor(data_no_outlier$job_title_grouped)
summary(data_no_outlier$job_title_grouped)
levels(data_no_outlier$job_title_grouped)

#agrupar los precios por rangos
data_no_outlier["quartile"] <- cut(data_no_outlier$salary_in_usd, breaks=c(0, 85000,140000, Inf), labels=paste("Salary", 1:3, sep=""))

str(data_no_outlier)
head(data_no_outlier)
summary(data_no_outlier)
max(data_no_outlier$salary_in_usd)
```

At first we will try using a linear model to predict the salary in usd, and as we observed, it fits better by transforming this variable with log function.
```{r}
summary(data_no_outlier)
summary(lm(formula = log(salary_in_usd) ~ work_year + experience_level + job_title + remote_ratio + company_size + company_location, 
           data = data_no_outlier))
```

It worked pretty well, but we will try to get this problem to a classification one by trying to predict in which quartile a worker should be. At first we are creating a Naive Bayes and a Random Forest classifiers.
```{r}
set.seed(123)
data_no_outlier2 <- data_no_outlier[,!names(data_no_outlier) %in% c("salary_in_usd")]
trainIndex=createDataPartition(data_no_outlier2$quartile, p=0.7)$Resample1

train=data_no_outlier2[trainIndex, ]
test=data_no_outlier2[-trainIndex, ]
NBclassfier=naiveBayes(quartile~., data=train)
summary(train)
rf = randomForest(quartile~.- job_title - company_location - employment_type, data=train, proximity=TRUE)
```

```{r}
printALL=function(model){
  trainPred=predict(model, newdata = train, type = "class")
  trainTable=table(train$quartile, trainPred)
  testPred=predict(model, newdata=test, type="class")
  testTable=table(test$quartile, testPred)
  trainAcc=(trainTable[1,1]+trainTable[2,2]+trainTable[3,3])/sum(trainTable)
  testAcc=(testTable[1,1]+testTable[2,2]+testTable[3,3])/sum(testTable)
  message("Contingency Table for Training Data")
  print(trainTable)
  message("Contingency Table for Test Data")
  print(testTable)
  message("Accuracy")
  print(round(cbind(trainAccuracy=trainAcc, testAccuracy=testAcc),4))
}
printALL(NBclassfier)
printALL(rf)
```

Get the dataset with "quartile" column as a binary class column, by creating different datasets with the same data.
```{r}
LowQuartileDS <- train[,!names(train) %in% c("employment_type")]
LowQuartileDS$quartile2[LowQuartileDS$quartile == "Salary1"] <- 1
LowQuartileDS$quartile2[LowQuartileDS$quartile != "Salary1"] <- 0
LowQuartileDS <- LowQuartileDS[,!names(LowQuartileDS) %in% c("quartile", "job_title", "company_location")]

MLQuartileDS <- train[,!names(train) %in% c("employment_type")]
MLQuartileDS$quartile2[MLQuartileDS$quartile == "Salary2"] <- 1
MLQuartileDS$quartile2[MLQuartileDS$quartile != "Salary2"] <- 0
MLQuartileDS <- MLQuartileDS[,!names(MLQuartileDS) %in% c("quartile", "job_title", "company_location")]

MHQuartileDS <- train[,!names(train) %in% c("employment_type")]
MHQuartileDS$quartile2[MHQuartileDS$quartile == "Salary3"] <- 1
MHQuartileDS$quartile2[MHQuartileDS$quartile != "Salary3"] <- 0
MHQuartileDS <- MHQuartileDS[,!names(MHQuartileDS) %in% c("quartile", "job_title", "company_location")]
```

Use a logistic regression for each of the dataset that were just generated, so we can iterate over the test set and get the model it fits the best.
```{r}
LowModel <- glm(quartile2~., data = LowQuartileDS)
MLModel <- glm(quartile2~., data = MLQuartileDS)
MHModel <- glm(quartile2~., data = MHQuartileDS)

truePredictions = 0
for(i in 1:nrow(test)) {       # for-loop over rows
  testRow <- test[i, ]
  
  predictLow <- predict(LowModel, testRow, type = "response")
  predictML <- predict(MLModel, testRow, type = "response")
  predictMH <- predict(MHModel, testRow, type = "response")
  
  max = 0
  labels = c("Salary1", "Salary2", "Salary3")
  indice = 0
  predicciones = c(predictLow, predictML, predictMH, predictHigh)
  for(i in 1:3) {
    if(predicciones[i] > max) {
      max = predicciones[i]
      indice = i
    }
  }
  if(labels[indice] == testRow$quartile) {
    truePredictions = truePredictions + 1
  }
}
message("Accuracy")
print(truePredictions/nrow(test))
```

Try a logistic regression building a tiny neuronal network with nnet package that RStudio provides, instead of building it ourselves manually.
```{r}
# Fit the model
model <- nnet::multinom(quartile ~., data = train)
# Summarize the model
summary(model)
# Make predictions
prediccion <- model %>% predict(test)
head(prediccion)
# Model accuracy
mean(prediccion == test$quartile)
```

Build a decision tree model to check how it performs in this case by classifying our data in different quartiles.
```{r}
tree <- rpart(quartile ~.- company_location - job_title, data = train)
rpart.plot(tree)
printcp(tree)
printALL(tree)
```


#CLUSTERING

##Manual Distance matrix
```{r}
# Creamos los vectores que formarÃ¡n la matriz de distancia (0 son iguales los trabajos, 1 son opuestos)
Machine_Learning <- c(0.25,	0.5,	0.5,	0.4,	0.4,	0.85,	0.5,	0.5, 0)
Data_Specialist <- c(0.75,	0.4,	0.3,	0.3,	0.3,	0.8,	0.15, 0, 0.5)
Data_Scientist <- c(0.75,	0.3,	0.15,	0.25,	0.3,	0.8, 0, 0.15, 0.5)
CPO <- c(0.9,	0.75,	0.75,	0.75,	0.8, 0, 0.8,0.8,0.85)
Data_Engineer <- c(0.8,	0.4,	0.25,	0.1, 0,0.8,0.3,0.3,0.4)
Data_Architect <- c(0.75,	0.4,	0.25, 0,0.1,0.75,0.25,0.3,0.4)
Data_Analyst <- c(0.75,	0.3, 0,0.25,0.25,0.75,0.15,0.3,0.5)
Business_Intelligence <- c(0.75, 0, 0.3,0.4,0.4,0.75,0.3,0.4,0.5)
Computer_Vision <- c(0, 0.75,0.75,0.75,0.8,0.9,0.75,0.75,0.25)

D <- c(Computer_Vision, Business_Intelligence, Data_Analyst, Data_Architect, Data_Engineer, CPO, Data_Scientist, Data_Specialist, Machine_Learning)

My_Matrix <- matrix(D, byrow=TRUE, nrow=9)
rownames(My_Matrix) <- c("Computer_Vision", "Business_Intelligence", "Data_Analyst", "Data_Architect", "Data_Engineer", "CPO", "Data_Scientist", "Data_Specialist", "Machine_Learning")
colnames(My_Matrix) <- c("Computer_Vision", "Business_Intelligence", "Data_Analyst", "Data_Architect", "Data_Engineer", "CPO", "Data_Scientist", "Data_Specialist", "Machine_Learning")

My_Matrix
```

##Plotted Distance Matrix
```{r}
Distance_Matrix <- as.dist(My_Matrix)

mds.coor <- cmdscale(Distance_Matrix)
plot(mds.coor[,2], mds.coor[,2], type="n", xlab="", ylab="")
text(jitter(mds.coor[,1]), jitter(mds.coor[,2]), rownames(mds.coor), cex=0.8, col = c("#FF0000", "#0000FF", "#00FF00", "#FF00FF", "#FFFF00", "#00CCCC", "#000000", "#999999", "#9900FF", "#009966"))
abline(h=0,v=0,col="gray75")
```
##Hierarchical Clustering using the Distance matrix of the Job Titles
```{r}
hc <- hclust(Distance_Matrix)
dend <-set(as.dendrogram(hc), "branches_lwd", 4)
d1=color_branches(dend,k=5, col = c(3,1,1,4,1))
d2=color_branches(d1,k=5) # auto-coloring 5 clusters of branches.
par(mar = c(9, 4, 4, 2) + 0.1)
plot(d2, lwd=2)
```

##K-Means using Distance matrix of the Job Titles
```{r}
kmeans.re <- kmeans(Distance_Matrix, centers = 4, nstart = 20)

fviz_cluster(kmeans.re, Distance_Matrix,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#00FFFF"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```