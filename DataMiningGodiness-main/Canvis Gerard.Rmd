---
title: "Main"
output:
  html_document: default
  pdf_document: default
date: '2022-12-11'
---

```{r setup, include=FALSE, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
knitr::opts_chunk$set(echo = TRUE)
library('corrplot')
library('ggplot2')
library('Hmisc')
library('moments')
library('dendextend')
library('countrycode')
library('ggthemes') # Load
library('dplyr')
library('tidyverse')
library('cluster')
library('factoextra')
library('rpart')
library('caret')
library('rpart.plot')
library('rattle')
library('partykit')
library('tibble')
library('purrr')
library('data.tree')
```

```{r}
salaries= read.csv("AI_MLsalaries.csv")
my_data <- salaries
```

```{r}
#summarizing the data
str(my_data)
summary(my_data)
```

```{r}
auxiliar <- my_data
my_data[sapply(my_data, is.character)] <- data.matrix(my_data[sapply(my_data, is.character)])
summary(my_data)
plot(my_data$salary_in_usd)
hist(my_data$salary_in_usd)
```


```{r}
#removing outliers
boxplot(my_data$salary_in_usd)
quartiles1 <- quantile(my_data$salary_in_usd, probs=c(.01, .90), na.rm = FALSE)
IQR <- IQR(my_data$salary_in_usd)
 
Lower1 <- quartiles1[1] - 1.5*IQR
Upper1 <- quartiles1[2] + 1.5*IQR
 
data_no_outlier <- subset(my_data,my_data$salary_in_usd > Lower1 & my_data$salary_in_usd < Upper1)
boxplot(data_no_outlier$salary_in_usd)
hist(data_no_outlier$salary_in_usd)
```


```{r}
plot(data_no_outlier$salary_in_usd)
```

```{r}
salaTitle <-  data_no_outlier[, c(4, 5)]
summary(salaTitle)
plot(salaTitle)
```



```{r}
summary(data_no_outlier$experience_level)
summary(data_no_outlier$employee_residence)
summary(data_no_outlier$employment_type)
summary(data_no_outlier$company_size)
summary(data_no_outlier$remote_ratio)
summary(data_no_outlier$job_title)
```

plot de los salarios por experiencia diferenciado en años
```{r}
plot(data_no_outlier$company_size, data_no_outlier$salary_in_usd , col=data_no_outlier$company_size, xlab="company_size", ylab="salary" )
```

```{r}
corrplot(cor(data_no_outlier), method = "color", type = "upper")
```

```{r}
#reiniciar los datos
data_no_outlier <- read.csv("AI_MLsalaries.csv",stringsAsFactors = TRUE)

#removing outliers
quartiles1 <- quantile(data_no_outlier$salary_in_usd, probs=c(.01, .90), na.rm = FALSE)
IQR <- IQR(data_no_outlier$salary_in_usd)
 
Lower1 <- quartiles1[1] - 1.5*IQR
Upper1 <- quartiles1[2] + 1.5*IQR
 
data_no_outlier <- subset(data_no_outlier,data_no_outlier$salary_in_usd > Lower1 & data_no_outlier$salary_in_usd < Upper1)

#eliminating columns
data_no_outlier["salary"] <- NULL
data_no_outlier["employee_residence"] <- NULL
data_no_outlier["salary_currency"] <- NULL

#eliminating not FT
head(data_no_outlier)
data_no_outlier <- data_no_outlier[data_no_outlier$employment_type == "FT",]

#remote_ratio as factor
data_no_outlier["remote_ratio"] <- as.factor(data_no_outlier$remote_ratio)
data_no_outlier["work_year"] <- as.factor(data_no_outlier$work_year)
#NR -> no remote work, PR -> partially remote, FR -> fully remtote
levels(data_no_outlier$remote_ratio) <- list(NR  = "0", PR = "50", FR = "100")

#grouping by continents
data_no_outlier$continent <- countrycode(sourcevar = data_no_outlier[, "company_location"],
                            origin = "iso2c",
                            destination = "continent")
data_no_outlier$continent[data_no_outlier$company_location == "CA" | data_no_outlier$company_location == "US"] <- "North America"
data_no_outlier$continent = as.factor(data_no_outlier$continent)

#gruouping jobs (for clustering approach)
data_no_outlier$job_title_grouped <- data_no_outlier$job_title
data_no_outlier[grepl("BI", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "BI Analyst"
data_no_outlier[grepl("Data Analy", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Analyst"
data_no_outlier[grepl("Sci", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Scientist"
data_no_outlier[grepl("Machine Learning", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "ML Engineer"
data_no_outlier[grepl("Data Engi", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("NLP", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Analytics", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Research", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("ETL", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Data Operations", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Engineer"
data_no_outlier[grepl("Computer Vision", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "3D Computer Vision Researcher"
data_no_outlier[grepl("Data Architect", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Architect"
data_no_outlier[grepl("Head of", data_no_outlier$job_title_grouped, fixed=TRUE),]$job_title_grouped <- "Data Manager"
data_no_outlier$job_title_grouped <- factor(data_no_outlier$job_title_grouped)
summary(data_no_outlier$job_title_grouped)
levels(data_no_outlier$job_title_grouped)

#agrupar los precios por rangos
data_no_outlier$quartile <- ntile(data_no_outlier$salary_in_usd, 4)
data_no_outlier["quartile"] <- as.factor(data_no_outlier$quartile)
levels(data_no_outlier$quartile) <- list(Low  = "1", Medium_low = "2", Medium_high = "3", High = "4")

str(data_no_outlier)
head(data_no_outlier)
```



```{r}
summary(data_no_outlier)
summary(lm(formula = salary_in_usd ~ work_year + experience_level + job_title + remote_ratio + company_size + company_location, 
           data = data_no_outlier))
```

#CLUSTERING

##Manual Distance matrix
```{r}
# Creamos los vectores que formarÃ¡n la matriz de distancia (0 son iguales los trabajos, 1 son opuestos)
Machine_Learning <- c(0.25,	0.5,	0.5,	0.4,	0.4,	0.85,	0.5,	0.5, 0)
Data_Specialist <- c(0.75,	0.4,	0.3,	0.3,	0.3,	0.8,	0.15, 0, 0.5)
Data_Scientist <- c(0.75,	0.3,	0.15,	0.25,	0.3,	0.8, 0, 0.15, 0.5)
CPO <- c(0.9,	0.75,	0.75,	0.75,	0.8, 0, 0.8,0.8,0.85)
Data_Engineer <- c(0.8,	0.4,	0.25,	0.1, 0,0.8,0.3,0.3,0.4)
Data_Architect <- c(0.75,	0.4,	0.25, 0,0.1,0.75,0.25,0.3,0.4)
Data_Analyst <- c(0.75,	0.3, 0,0.25,0.25,0.75,0.15,0.3,0.5)
Business_Intelligence <- c(0.75, 0, 0.3,0.4,0.4,0.75,0.3,0.4,0.5)
Computer_Vision <- c(0, 0.75,0.75,0.75,0.8,0.9,0.75,0.75,0.25)

D <- c(Computer_Vision, Business_Intelligence, Data_Analyst, Data_Architect, Data_Engineer, CPO, Data_Scientist, Data_Specialist, Machine_Learning)

My_Matrix <- matrix(D, byrow=TRUE, nrow=9)
rownames(My_Matrix) <- c("Computer_Vision", "Business_Intelligence", "Data_Analyst", "Data_Architect", "Data_Engineer", "CPO", "Data_Scientist", "Data_Specialist", "Machine_Learning")
colnames(My_Matrix) <- c("Computer_Vision", "Business_Intelligence", "Data_Analyst", "Data_Architect", "Data_Engineer", "CPO", "Data_Scientist", "Data_Specialist", "Machine_Learning")

My_Matrix
```

##Plotted Distance Matrix
```{r}
Distance_Matrix <- as.dist(My_Matrix)

mds.coor <- cmdscale(Distance_Matrix)
plot(mds.coor[,2], mds.coor[,2], type="n", xlab="", ylab="")
text(jitter(mds.coor[,1]), jitter(mds.coor[,2]), rownames(mds.coor), cex=0.8, col = c("#FF0000", "#0000FF", "#00FF00", "#FF00FF", "#FFFF00", "#00CCCC", "#000000", "#999999", "#9900FF", "#009966"))
abline(h=0,v=0,col="gray75")
```
##Hierarchical Clustering using the Distance matrix of the Job Titles
```{r}
hc <- hclust(Distance_Matrix)
dend <-set(as.dendrogram(hc), "branches_lwd", 4)
d1=color_branches(dend,k=5, col = c(3,1,1,4,1))
d2=color_branches(d1,k=5) # auto-coloring 5 clusters of branches.
par(mar = c(9, 4, 4, 2) + 0.1)
plot(d2, lwd=2)
```

##K-Means using Distance matrix of the Job Titles
```{r}
kmeans.re <- kmeans(Distance_Matrix, centers = 4, nstart = 20)

fviz_cluster(kmeans.re, Distance_Matrix,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#00FFFF"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```

##ID3 Decision tree
```{r}
#define a function to calculate the entropy of a categorical variable
# entropy <- function(target) {
#   levels <- levels(target)
#   n <- length(target)
#   sum_of_probs <- 0
#   
#   for (i in 1:length(levels)) {
#     level <- levels[i]
#     p <- sum(target == level)/n
#     if (p > 0) sum_of_probs <- sum_of_probs - p * log2(p)
#   }
#   
#   return(sum_of_probs)
# }
# 
# #define a function to calculate the information gain of a split
# info_Gain <- function(data, feature, target) {
#   data$feature <- as.factor(data[, feature])
#   levels <- levels(data$feature)
#   n <- nrow(data)
#   sum_of_entropies <- 0
#   
#   for (i in 1:length(levels)) {
#     level <- levels[i]
#     sum_of_entropies <- sum_of_entropies + (table(data$feature)[level]/n) * entropy(data[, target][data$feature == level])
#   }
#   
#   return(sum_of_entropies)
# }
# 
# #define a function to create the ID3 tree
# id3 <- function(data, target, predictors) {
#   
#   #if all the target variable values are the same, return the class label
#   if (length(unique(data[, target])) == 1) {
#     return(list(class = unique(data[, target])))
#   }
#   
#   #if there are no more predictors, return the most common class label
#   if (length(predictors) == 0) {
#     return(list(class = names(sort(-table(data[, target])))[1]))
#   }
#   
#   #choose the best predictor based on information gain
#   best_predictor <- predictors[which.max(sapply(predictors, function(x) info_gain(data, x, target)))]
#   
#   #create a new node for the tree
#   tree <- list(predictor = best_predictor, children = list())
#   
#   #split the data based on the values of the best predictor
#   levels <- unique(data[, best_predictor])
#   for (level in levels) {
#     child_data <- data[data[, best_predictor] == level, ]
#     child_predictors <- predictors[predictors != best_predictor]
#     tree$children[[level]] <- id3(child_data, target, child_predictors)
#   }
#   
#   return(tree)
# }
# 
# target <- "quartile"
# predictors <- colnames(data_no_outlier)[!colnames(data_no_outlier) %in% c("salary_in_usd", "quartile")]
# 
# #construct the tree
# tree <- id3(data_no_outlier, target, predictors)
# 
# #plot the tree
# party_tree <- as.party(tree)
# plot(party_tree)
```


```{r, include=FALSE}
# entropy <- function(q) {
#   # Calculate the entropy for a value.
#   -1 * (q * log2(q) + (1 - q) * log2(1 - q))
# }
# 
# positiveRatio <- function(data, outcomeCol = ncol(data)) {
#   # Calculate the ratio of T by the total samples.
#   positiveCount <- length(which(data[, outcomeCol] == T))
#   sum(positiveCount / nrow(data))
# }
# 
# gain <- function(data, attributeCol, outcomeCol = ncol(data), precision=3) {
#   # Calculate the information gain for an attribute.
#   # First, calculate the total entropy for this attribute by using its positive ratio.
#   systemEntropy <- round(entropy(positiveRatio(data, outcomeCol)), precision)
# 
#   # Get the list of all T and all F outcomes.
#   positives <- data[which(data[,outcomeCol] == T),]
#   negatives <- data[which(data[,outcomeCol] == F),]
#   
#   # Split the attribute into groups by its possible values (sunny, overcast, rainy).
#   attributeValues <- split(data, data[,attributeCol])
#   
#   # Sum the entropy for each positive attribute value.
#   gains <- sum(sapply(attributeValues, function(attributeValue) {
#     # Calculate the ratio for this attribute value by all measurements.
#     itemRatio <- nrow(attributeValue) / nrow(data)
#     
#     # Calculate the entropy for this attribute value.
#     outcomeEntropy <- entropy(length(which(attributeValue[,outcomeCol] == T)) / nrow(attributeValue))
#     
#     # Cast NaN to 0 and return the result.
#     result <- itemRatio * outcomeEntropy
#     round(ifelse(is.nan(result), 0, result), precision)
#   }))
#   
#   # The information gain is the remainder from the attribute entropy minus the attribute value gains.
#   systemEntropy - gains
# }
# 
# pure <- function(data, outcomeCol = ncol(data)) {
#   length(unique(data[, outcomeCol])) == 1
# }
# 
# ID3 <- function(node, data, outcomeCol = ncol(data)) {
#   node$obsCount <- nrow(data)
#   
#   # If the data-set contains all the same outcome values, then make a leaf.
#   if (pure(data, outcomeCol)) {
#     # Construct a leaf having the name of the attribute value.
#     child <- node$AddChild(unique(data[,outcomeCol]))
#     node$feature <- tail(names(data), 1)
#     child$obsCount <- nrow(data)
#     child$feature <- ''
#   }
#   else {
#     # Chose the attribute with the highest information gain.
#     gains <- sapply(colnames(data)[-outcomeCol], function(colName) {
#       gain(data, which(colnames(data) == colName), outcomeCol)
#     })
#     
#     feature <- names(gains)[gains == max(gains)][1]
#     
#     node$feature <- feature
#     
#     # Take the subset of the data-set having that attribute value.
#     childObs <- split(data[,!(names(data) %in% feature)], data[,feature], drop = TRUE)
#     
#     for(i in 1:length(childObs)) {
#       # Construct a child having the name of that attribute value.
#       child <- node$AddChild(names(childObs)[i])
#       
#       # Call the algorithm recursively on the child and the subset.
#       ID3(child, childObs[[i]])
#     }
#   }
# }
# 
# # Test calculating information gain for all columns.
# sapply(1:ncol(data_no_outlier), function(i) { print(gain(data_no_outlier, i)) })
# 
# # Train ID3 to build a decision tree.
# tree <- Node$new('Should_Play')
# ID3(tree, data_no_outlier)
# print(tree, 'feature')
```

```{r}
# Specify the target variable and predictors
target <- "quartile"
predictors <- colnames(data_no_outlier)[!colnames(data_no_outlier) %in% c("salary_in_usd", "quartile", "job_title", "company_location")]

# Fit the decision tree model using rpart
model <- rpart(target ~ ., data = data_no_outlier[, c(target, predictors)], method = "class")

# Plot the decision tree
plot(model)
text(model)
```


